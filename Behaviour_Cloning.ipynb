{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kRLuwhxZBcT"
      },
      "source": [
        "**Implement Behaviour Cloning using RL along with Direct policy Learning and Inverse RL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwJ1LHlN9E33",
        "outputId": "8a3529c4-552d-4e29-9690-c7d6676553ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (748 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.11.0+cu113)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.4\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 22.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3[extra]) (4.11.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (4.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.46.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616825 sha256=e6b4b873bf52f02d5d0f4d4d62510d855299f76455c7f23105d3957f31493e63\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=3c8b0f14dacbbd79dc3aa75d71a5694460bf4416da428f66dadbc35ec37b2326\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, AutoROM.accept-rom-license, autorom, stable-baselines3, ale-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0 stable-baselines3-1.5.0\n"
          ]
        }
      ],
      "source": [
        "# For Box2D env\n",
        "!apt-get install swig\n",
        "!pip install gym[box2d]\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgx4AMZo8anP"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_1ZNBum8ane"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaz2Szrl8anx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS9lr70R9eU9"
      },
      "outputs": [],
      "source": [
        "# Example for continuous actions\n",
        "# env_id = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "# Example for discrete actions\n",
        "env_id = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh88d4oR8an6"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKCgHCc_8aoB"
      },
      "source": [
        "## Train Expert Model\n",
        "\n",
        "We create an expert RL agent and let it learn to solve a task by interacting with the evironment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkmIST0r8aoC",
        "outputId": "9a565f9c-2d47-44e4-b27c-3b979d594000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 21.9     |\n",
            "|    ep_rew_mean     | 21.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 1499     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 25.6        |\n",
            "|    ep_rew_mean          | 25.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1165        |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 3           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008541806 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.686      |\n",
            "|    explained_variance   | -0.00171    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.55        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    value_loss           | 51.4        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 32.4       |\n",
            "|    ep_rew_mean          | 32.4       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1078       |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 5          |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00940156 |\n",
            "|    clip_fraction        | 0.053      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.665     |\n",
            "|    explained_variance   | 0.1        |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 13         |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0144    |\n",
            "|    value_loss           | 37.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 44.8        |\n",
            "|    ep_rew_mean          | 44.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 938         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009063646 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.631      |\n",
            "|    explained_variance   | 0.226       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.9        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0199     |\n",
            "|    value_loss           | 52          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=341.80 +/- 119.83\n",
            "Episode length: 341.80 +/- 119.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 342         |\n",
            "|    mean_reward          | 342         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 10000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008334938 |\n",
            "|    clip_fraction        | 0.0688      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.616      |\n",
            "|    explained_variance   | 0.271       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 23.8        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0174     |\n",
            "|    value_loss           | 60.8        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 58.9     |\n",
            "|    ep_rew_mean     | 58.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 649      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 15       |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 76.3        |\n",
            "|    ep_rew_mean          | 76.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 690         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010561302 |\n",
            "|    clip_fraction        | 0.0816      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.589      |\n",
            "|    explained_variance   | 0.429       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.5        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0137     |\n",
            "|    value_loss           | 65.1        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 94.5         |\n",
            "|    ep_rew_mean          | 94.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 722          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060716844 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.574       |\n",
            "|    explained_variance   | 0.482        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.7         |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00763     |\n",
            "|    value_loss           | 52.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 112          |\n",
            "|    ep_rew_mean          | 112          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 732          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073174844 |\n",
            "|    clip_fraction        | 0.0335       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.571       |\n",
            "|    explained_variance   | 0.81         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.24         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    value_loss           | 31.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 130          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 750          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025969583 |\n",
            "|    clip_fraction        | 0.024        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.544       |\n",
            "|    explained_variance   | 0.777        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.51         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00362     |\n",
            "|    value_loss           | 45.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=478.40 +/- 30.19\n",
            "Episode length: 478.40 +/- 30.19\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 478          |\n",
            "|    mean_reward          | 478          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 20000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068984795 |\n",
            "|    clip_fraction        | 0.0749       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.556       |\n",
            "|    explained_variance   | 0.931        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.84         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00994     |\n",
            "|    value_loss           | 16           |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 146      |\n",
            "|    ep_rew_mean     | 146      |\n",
            "| time/              |          |\n",
            "|    fps             | 737      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 27       |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 164          |\n",
            "|    ep_rew_mean          | 164          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 755          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070077237 |\n",
            "|    clip_fraction        | 0.0662       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.545       |\n",
            "|    explained_variance   | 0.789        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 26.1         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00609     |\n",
            "|    value_loss           | 40.1         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 183         |\n",
            "|    ep_rew_mean          | 183         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 769         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 31          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004215927 |\n",
            "|    clip_fraction        | 0.0328      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.53       |\n",
            "|    explained_variance   | 0.892       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.42        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00687    |\n",
            "|    value_loss           | 29          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 199          |\n",
            "|    ep_rew_mean          | 199          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 783          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077156303 |\n",
            "|    clip_fraction        | 0.0778       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.537       |\n",
            "|    explained_variance   | 0.962        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.987        |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.0107      |\n",
            "|    value_loss           | 9.21         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 219          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 794          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054160864 |\n",
            "|    clip_fraction        | 0.0648       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.537       |\n",
            "|    explained_variance   | 0.902        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.651        |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.0122      |\n",
            "|    value_loss           | 4.41         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 500         |\n",
            "|    mean_reward          | 500         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 30000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010173936 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.502      |\n",
            "|    explained_variance   | -0.00102    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.37        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00652    |\n",
            "|    value_loss           | 26          |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 237      |\n",
            "|    ep_rew_mean     | 237      |\n",
            "| time/              |          |\n",
            "|    fps             | 778      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 39       |\n",
            "|    total_timesteps | 30720    |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "ppo_expert = PPO('MlpPolicy', env_id, verbose=1, create_eval_env=True)\n",
        "ppo_expert.learn(total_timesteps=3e4, eval_freq=10000)\n",
        "ppo_expert.save(\"ppo_expert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlyTfGAQ_Az1"
      },
      "source": [
        "check the performance of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_rVEjC0_AQa",
        "outputId": "3a9eab92-ed1b-4240-80a9-ff124137f772"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = 500.0 +/- 0.0\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Kv6v_V8aoJ"
      },
      "source": [
        "## Create Student\n",
        "\n",
        "We also create a student RL agent, which will later be trained with the expert dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLdLPUeC8aoL",
        "outputId": "28cc54dc-afe7-4ba3-a45d-21f0422c0ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "a2c_student = A2C('MlpPolicy', env_id, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3GuNxcU8aoT"
      },
      "source": [
        "\n",
        "We now let our expert interact with the environment (except we already have expert data) and store resultant expert observations and actions to build an expert dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emodyZDW8aoU"
      },
      "outputs": [],
      "source": [
        "num_interactions = int(4e4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3I_2s808aoZ",
        "outputId": "2bb07e32-85b9-4c21-d66f-e0402c6f6335"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [00:16<00:00, 2380.95it/s]\n"
          ]
        }
      ],
      "source": [
        "if isinstance(env.action_space, gym.spaces.Box):\n",
        "  expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "  expert_actions = np.empty((num_interactions,) + (env.action_space.shape[0],))\n",
        "\n",
        "else:\n",
        "  expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "  expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "for i in tqdm(range(num_interactions)):\n",
        "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
        "    expert_observations[i] = obs\n",
        "    expert_actions[i] = action\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "\n",
        "np.savez_compressed(\n",
        "    \"expert_data\",\n",
        "    expert_actions=expert_actions,\n",
        "    expert_observations=expert_observations,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT72bR1i8aog"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUetr5vl8aom"
      },
      "outputs": [],
      "source": [
        "class ExpertDataSet(Dataset):\n",
        "    def __init__(self, expert_observations, expert_actions):\n",
        "        self.observations = expert_observations\n",
        "        self.actions = expert_actions\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.observations[index], self.actions[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bNAhXp8aor"
      },
      "source": [
        "\n",
        "\n",
        "We now instantiate the `ExpertDataSet` and split it into training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIdT-zMV8aot"
      },
      "outputs": [],
      "source": [
        "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
        "\n",
        "train_size = int(0.8 * len(expert_dataset))\n",
        "\n",
        "test_size = len(expert_dataset) - train_size\n",
        "\n",
        "train_expert_dataset, test_expert_dataset = random_split(\n",
        "    expert_dataset, [train_size, test_size]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LgmtFFq8aox",
        "outputId": "569b3495-b8fd-41f2-a93f-28203e36bd3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_expert_dataset:  8000\n",
            "train_expert_dataset:  32000\n"
          ]
        }
      ],
      "source": [
        "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
        "print(\"train_expert_dataset: \", len(train_expert_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwUhCTGU8ao5"
      },
      "outputs": [],
      "source": [
        "def pretrain_agent(\n",
        "    student,\n",
        "    batch_size=64,\n",
        "    epochs=1000,\n",
        "    scheduler_gamma=0.7,\n",
        "    learning_rate=1.0,\n",
        "    log_interval=100,\n",
        "    no_cuda=True,\n",
        "    seed=1,\n",
        "    test_batch_size=64,\n",
        "):\n",
        "    use_cuda = not no_cuda and th.cuda.is_available()\n",
        "    th.manual_seed(seed)\n",
        "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "    if isinstance(env.action_space, gym.spaces.Box):\n",
        "      criterion = nn.MSELoss()\n",
        "    else:\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Extract initial policy\n",
        "    model = student.policy.to(device)\n",
        "\n",
        "    def train(model, device, train_loader, optimizer):\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if isinstance(env.action_space, gym.spaces.Box):\n",
        "              # A2C/PPO policy outputs actions, values, log_prob\n",
        "              # SAC/TD3 policy outputs actions only\n",
        "              if isinstance(student, (A2C, PPO)):\n",
        "                action, _, _ = model(data)\n",
        "              else:\n",
        "                # SAC/TD3:\n",
        "                action = model(data)\n",
        "              action_prediction = action.double()\n",
        "            else:\n",
        "              # Retrieve the logits for A2C/PPO when using discrete actions\n",
        "              dist = model.get_distribution(data)\n",
        "              action_prediction = dist.distribution.logits\n",
        "              target = target.long()\n",
        "\n",
        "            loss = criterion(action_prediction, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(\n",
        "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                        epoch,\n",
        "                        batch_idx * len(data),\n",
        "                        len(train_loader.dataset),\n",
        "                        100.0 * batch_idx / len(train_loader),\n",
        "                        loss.item(),\n",
        "                    )\n",
        "                )\n",
        "    def test(model, device, test_loader):\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with th.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                if isinstance(env.action_space, gym.spaces.Box):\n",
        "                  # A2C/PPO policy outputs actions, values, log_prob\n",
        "                  # SAC/TD3 policy outputs actions only\n",
        "                  if isinstance(student, (A2C, PPO)):\n",
        "                    action, _, _ = model(data)\n",
        "                  else:\n",
        "                    # SAC/TD3:\n",
        "                    action = model(data)\n",
        "                  action_prediction = action.double()\n",
        "                else:\n",
        "                  # Retrieve the logits for A2C/PPO when using discrete actions\n",
        "                  dist = model.get_distribution(data)\n",
        "                  action_prediction = dist.distribution.logits\n",
        "                  target = target.long()\n",
        "\n",
        "                test_loss = criterion(action_prediction, target)\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
        "    # and testing\n",
        "    train_loader = th.utils.data.DataLoader(\n",
        "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
        "    )\n",
        "    test_loader = th.utils.data.DataLoader(\n",
        "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
        "    )\n",
        "\n",
        "    # Define an Optimizer and a learning rate schedule.\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
        "\n",
        "    # Now we are finally ready to train the policy model.\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, optimizer)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    # Implant the trained policy network back into the RL student agent\n",
        "    a2c_student.policy = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkEP6i0hEu_R"
      },
      "source": [
        "Evaluate the agent before pretraining, it should be random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7kvYIneEui8",
        "outputId": "94edd292-8c44-40d9-e17c-9611adafe967"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = 98.8 +/- 11.98999582985749\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgduZAbF8ao9"
      },
      "source": [
        "\n",
        "\n",
        "Having defined the training procedure we can now run the pretraining!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI1EFFnW8ao-",
        "outputId": "88a2c61a-f332-4f4f-c572-5440020e00b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/32000 (0%)]\tLoss: 0.692737\n",
            "Train Epoch: 1 [6400/32000 (20%)]\tLoss: 0.311955\n",
            "Train Epoch: 1 [12800/32000 (40%)]\tLoss: 0.260270\n",
            "Train Epoch: 1 [19200/32000 (60%)]\tLoss: 0.257318\n",
            "Train Epoch: 1 [25600/32000 (80%)]\tLoss: 0.198477\n",
            "Test set: Average loss: 0.0000\n",
            "Train Epoch: 2 [0/32000 (0%)]\tLoss: 0.258305\n",
            "Train Epoch: 2 [6400/32000 (20%)]\tLoss: 0.222526\n",
            "Train Epoch: 2 [12800/32000 (40%)]\tLoss: 0.237402\n",
            "Train Epoch: 2 [19200/32000 (60%)]\tLoss: 0.096135\n",
            "Train Epoch: 2 [25600/32000 (80%)]\tLoss: 0.097110\n",
            "Test set: Average loss: 0.0000\n",
            "Train Epoch: 3 [0/32000 (0%)]\tLoss: 0.098809\n",
            "Train Epoch: 3 [6400/32000 (20%)]\tLoss: 0.142287\n",
            "Train Epoch: 3 [12800/32000 (40%)]\tLoss: 0.075540\n",
            "Train Epoch: 3 [19200/32000 (60%)]\tLoss: 0.094173\n",
            "Train Epoch: 3 [25600/32000 (80%)]\tLoss: 0.093501\n",
            "Test set: Average loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "pretrain_agent(\n",
        "    a2c_student,\n",
        "    epochs=3,\n",
        "    scheduler_gamma=0.7,\n",
        "    learning_rate=1.0,\n",
        "    log_interval=100,\n",
        "    no_cuda=True,\n",
        "    seed=1,\n",
        "    batch_size=64,\n",
        "    test_batch_size=1000,\n",
        ")\n",
        "a2c_student.save(\"a2c_student\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3Q5Jm58apE"
      },
      "source": [
        "\n",
        "\n",
        "Finally, let us test how well our RL agent student learned to mimic the behavior of the expert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKZ8O--m8apF",
        "outputId": "8b9a1b92-b656-48c7-b26a-f155eefecb42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = 500.0 +/- 0.0\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wsUJ9FfNCWB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N376OzGxaL5Y"
      },
      "source": [
        "Example Scenario:\n",
        "\n",
        "Let's consider a scenario where an autonomous vehicle needs to learn how to navigate through a city environment to reach a destination while obeying traffic rules and avoiding accidents. We'll use BC, RL with DPL, and IRL to tackle different aspects of this problem.\n",
        "\n",
        "1. Behavior Cloning (BC):\n",
        "\n",
        "In Behavior Cloning, we use expert demonstrations to train a model to mimic the expert's behavior. In our example, the expert could be a human driver providing demonstrations of safe and efficient driving behavior.\n",
        "\n",
        "Usage: We collect a dataset of expert demonstrations, consisting of observations (states) and corresponding actions (steering, acceleration, etc.). We then train a model, such as a neural network, using supervised learning to predict actions from states.\n",
        "\n",
        "\n",
        "2. Reinforcement Learning (RL) with Direct Policy Learning (DPL):\n",
        "\n",
        "Reinforcement Learning allows the autonomous vehicle to learn from its interaction with the environment through trial and error. Direct Policy Learning (DPL) involves learning the policy directly from the observed state-action pairs, without relying on explicit reward signals.\n",
        "\n",
        "Usage: We define the RL environment, including the city map, traffic rules, and other vehicles. The agent (autonomous vehicle) interacts with this environment, and we use techniques like Q-learning or Deep Q-Networks to learn a policy from scratch. With DPL, the agent directly updates its policy based on observed state-action pairs during training, without explicitly defined reward signals.\n",
        "\n",
        "\n",
        "3. Inverse RL (IRL):\n",
        "\n",
        "Inverse RL helps in learning the underlying reward function from observed behavior. In our scenario, it could help in inferring the implicit reward structure of safe and efficient driving from expert demonstrations or real-world data.\n",
        "\n",
        "Usage: We use IRL to infer the reward function that likely led to the expert's behavior. By analyzing the expert demonstrations, IRL estimates the underlying reward structure that encourages safe and efficient driving behavior. This inferred reward function can then be used to guide the RL agent's learning process, ensuring it learns to prioritize actions that lead to high rewards."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
